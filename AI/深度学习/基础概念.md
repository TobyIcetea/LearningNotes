# 基础概念

## 1. 神经网络

神经网络是一种模拟人脑神经系统结构的算法，用于处理复杂的非线性问题。其核心理念是通过多个简单的计算单元（即神经元）组成网络，并通过多层结构和大量训练数据，使模型逐渐学习到复杂的特征和模式。

### 1.1 神经元

- 神经元（Neuron），又称为结点或单元，是神经网络的基本构成单元。每个神经元接受输入，执行加权求和，然后通过一个激活函数产生输出。

- 数学上，神经元的输出可以表示为：
    $$
    y=f(\sum^n_{i=1}w_ix_i+b)
    $$
    其中 $x_i$ 是输入，$w_i$ 是权重，$b$ 是偏置项，$f$ 是激活函数。

### 1.2 激活函数

- 激活函数用于引入非线性，使网络可以处理复杂的非线性问题。
- 常见的激活函数包括：
    - Sigmoid 函数：输出值在 (0, 1) 之间，常用于二分类问题。
    - ReLU 函数：在正值区间输入与输出相等，负值区间输出 0，使得模型更高效。
    - Tanh 函数：输出在 (-1, 1) 之间，效果类似于 Sigmoid，但更适合于归一化的数据。

### 1.3 层结构

- 神经网络一般由输入层、隐藏层和输出层组成。
    - 输入层：接收输入数据，每个节点代表一个输入特征。
    - 隐藏层：位于输入和输出层之间，可以有多层，层数越多，网络越深，称为深度神经网络（DNN）。隐藏层中的神经元通过权重和激活函数计算特征。
    - 输出层：生成最终的预测结果，输出层节点的数量取决于任务（如分类的类别数）。

### 1.4 前向传播

- 前向传播（Forward Propagation）是指数据从输入层经过隐藏层逐层传递到输出层的过程。每层的输出作为下一层的输入，直至生成最终预测。

### 1.5 损失函数

- 损失函数（Loss Function）用于评估模型预测值和真实值之间的差距。
- 常见的损失函数包括：
    - 均方误差（MSE）：用于回归任务，计算预测值与真实值之间的平方差。
    - 交叉熵损失：用于分类任务，尤其是多类分类问题。

### 1.6 反向传播

- 反向传播（Backpropagation）是训练神经网络的核心算法。它通过梯度下降优化权重和偏置，以最小化损失函数。
- 反向传播算法从输出层开始，逐层计算损失对每个权重的导数，并利用链式法则想输入层反传这些误差，更新神经网络中的权重。

### 1.7 优化算法

- 优化算法决定了如何更新权重以最小化损失。
- 常见的优化算法有：
    - 梯度下降（SGD）：逐步沿着损失函数的梯度方向更新权重。
    - Adam：一种自适应学习率优化算法，结合了动量和 RMSprop 的优点，更适合复杂问题。

### 1.8 训练和测试

- 神经网络的训练数据分为训练集和测试集。
- 在训练集中调整权重，通过测试集评估模型的泛化能力，以避免过拟合（在训练集上表现好但在新数据上效果差）。

### 1.9 深度学习与卷积神经网络（CNN）

- 当网络层数较多时，我们通常称之为深度神经网络（Deep Neural Network，DNN）。
- 卷积神经网络（Convolutional Neural Network，CNN）是深度学习的一个重要模型，尤其适合处理图像数据。CNN 通过卷积操作提取空间特征，并减少参数量，提高计算效率。



- [ ] 卷积操作









