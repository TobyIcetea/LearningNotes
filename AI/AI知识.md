# AI 知识

## 1. Pandas

Pandas 是 Python 中用于数据分析和处理的强大库，在人工智能和机器学习领域中发挥着至关重要的作用。以下是关于 Pandas 在 Python 人工智能中的相关知识。

### 1.1 Pandas 简介

Pandas 提供了高性能、易于使用的数据结构和数据分析工具，主要包括：

- `Series`：一维标记数组，类似于一列数据。
- `DataFrame`：二维标记数据结构，类似于电子表格或 SQL 表格。

### 1.2 在人工智能中的作用

在人工智能项目中，数据是核心。Pandas 提供了丰富的功能来处理和分析数据：

- 数据读取和存储：支持 CSV、Excel、SQL、JSON 等多种格式。
- 数据清洗：处理缺失值、重复数据和异常值。
- 数据转换：数据类型转换、归一化、编码等。
- 数据合并和重塑：连接、合并、透视等操作。
- 统计分析：基本统计、分组汇总、时间序列分析。

### 1.3 关键功能和方法

- 读取数据

    ```python
    df = pd.read_csv('data.csv')
    ```

- 查看数据

    ```python
    df.head()
    df.info()
    df.describe()
    ```

- 处理缺失值

    ```python
    df.dropna()
    df.fillna(value)
    ```

- 数据选择和过滤

    ```python
    df['column_name']
    df.loc(row_index, 'column_name')
    df[df['column_name'] > value]
    ```

- 数据可视化

    ```python
    df.plot()
    ```

### 1.4 与机器学习的集成

Pandas 常与 Scikit-Learn 等机器学习库结合使用，用于：

- 特征工程：处理和转换特征以适应模型需求。
- 数据准备：将 DataFrame 转换为 Numpy 数组或其他格式，供模型训练。
- 结果分析：通过 Pandas 分析模型输出和评估指标。

### 1.5 示例：使用 Pandas 进行数据预处理

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# 读取数据
df = pd.read_csv('diabetes_dataset.csv')

# 数据清洗，na 就是 not available
df = df.dropna()

# 特征值和目标变量
# 特征：用于预测的输入变量，即模型的自变量。在我的数据集中，特征就是除了 Outcome 之外的所有列
# 目标变量：模型要预测的输出变量，即因变量。在我的数据集中，目标变量是 Outcome，表示是否患有糖尿病
X = df.drop('Outcome', axis=1)
y = df['Outcome']

# 数据分割：将数据集划分为训练集和测试集的过程
# 训练集（Training Set）：用于训练模型，学习数据中的模式和规律
# 测试集（Test Set）：用于评估模型的性能，测试模型在未见过的数据上的表现
# test_size=0.2 表示：将数据集按 8:2 的比例分割为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 模型训练：使用训练集的数据来让机器学习算法学习特征与目标变量之间的关系
# 随机森林分类器（Random Forest Classifier）是一种集成学习方法，具有较好的性能和鲁棒性
# 初始化模型
model = RandomForestClassifier()
# 使用训练集训练模型
model.fit(X_train, y_train)

# 模型评估：评估模型在测试集上的性能，了解模型的预测能力
accuracy = model.score(X_test, y_test)
print(f'模型准确率：{accuracy}')
```

### 1.6 注意事项

- 性能优化：对于大型数据集，考虑使用分块处理或更高效的数据结构。
- 数据类型：确保数据类型正确，避免影响计算和模型性能。
- 版本兼容性：注意 Pandas 版本与其他库的兼容性。

## 2. 模型如何存储

### 2.1 训练后的模型信息存储在哪里？

当调用 `model.fit(X_train, y_train)` 时，模型会根据训练数据学习参数，这些参数（如权重、决策规则等）被存储在 `model` 对象的内部属性中。因此，训练后的模型信息保存在 `model` 变量中。

### 2.2 之后如何直接使用训练好的模型？

如果我们想在之后的程序中直接使用训练好的模型，而不想每次都重新训练，可以将模型保存到磁盘上，然后在需要时加载。这种过程称为模型持久化（Model Persistence）。

### 2.3 如何保存和加载模型？

常用的保存和加载模型的方法有两种：

- 使用 `joblib` 模块
- 使用 `pickle` 模块

#### 2.3.1 使用 `joblib` 模块

`joblib` 特别适合保存大型的 numpy 数组和模型。

保存模型：

```python
import joblib

# 保存模型到文件
joblib.dump(model, 'trained_model.joblib')
```

加载模型：

```python
import joblib

# 从文件加载模型
model = joblib.load('trained_model.joblib')
```

#### 2.3.2 使用 `pickle` 模块

`pickle` 是 Python 内置的序列化模块，可用于保存任何 Python 对象。

保存模型：

```python
import pickle

# 保存模型到文件
with open('trained_model.pkl', 'wb') as file:
    pickle.dump(model, file)
```

加载模型：

```python
import pickle

# 从文件夹加载模型
with open('trained_model.pkl', 'rb') as file:
    model = pickle.load(file)
```

注意事项：

- 安全性：从不受信任的来源加载模型时，要谨慎使用 `pickle` 或 `joblib`，因为反序列化可能执行恶意代码。
- 版本兼容性：确保在保存和加载模型时使用的库版本一致，以避免兼容性问题。

### 2.4 使用保存的模型进行预测

一旦加载了模型，就可以像之前一样使用它进行预测，而无需重新训练。例如：

```python
# 假设已经加载了模型
# model = joblib.load('trained_model.joblib')

# 准备新数据（与训练数据的特征列一致）
new_data = [[6, 148, 72, 35, 0, 33.6, 0.627, 50]]

# 进行预测
prediction = model.predict(new_data)

print(f'预测结果：{prediction}')
```

### 2.5 为什么要保存模型？

- 节省时间和资源：训练模型可能需要耗费大量时间和计算资源，保存模型后可以避免重复训练。
- 部署模型：在生产环境中，可以直接加载模型来服务于实时预测请求。
- 复现结果：保存模型可以确保结果的可重复性，便于分享和合作。

### 2.6 完整的示例

以下是完整的流程，从训练模型到保存到加载模型，再到使用模型进行预测。

model_dump.py：

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import joblib

# 1. 读取数据
df = pd.read_csv('../diabetes_dataset.csv')

# 2. 特征和目标变量
X = df.drop('Outcome', axis=1)
y = df['Outcome']

# 3. 模型分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 4. 模型训练
model = RandomForestClassifier()
model.fit(X_train, y_train)

# 5. 模型评估
accuracy = model.score(X_test, y_test)
print(f'模型准确率: {accuracy}')

# 6. 保存模型
joblib.dump(model, 'trained_model.joblib')
```

model_load.py：

```python
import joblib
import pandas as pd

# 加载模型
model = joblib.load('trained_model.joblib')

feature_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',
                 'DiabetesPedigreeFunction', 'Age']

# 使用加载的模型进行预测
new_data = pd.DataFrame(
    [[1, 93, 70, 31, 0, 30.4, 0.315, 23], [1, 126, 60, 0, 0, 30.1, 0.349, 47], [5, 121, 72, 23, 112, 26.2, 0.245, 30]],
    columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',
             'DiabetesPedigreeFunction', 'Age'])
prediction = model.predict(new_data)
print(f'预测结果：{prediction}')
```

### 2.7 总结

- 模型信息存储：训练后的模型参数和结构保存在模型对象 `model` 中。
- 模型持久化：通过 `joblib` 和 `pickle` 可以将模型保存到磁盘，方便以后加载使用。
- 无需重新训练：加载已保存的模型后，可以直接使用它进行预测，无需重新训练。

## 3. 有监督学习和无监督学习

在 Python 人工智能中，有监督学习和无监督学习是两种主要的机器学习方法，每种方法在处理数据、训练模型以及应用场景上各有不同。

### 3.1 有监督学习

- 数据：有监督学习需要“标记”好的数据，意思是数据不仅包含输入，还包含正确答案（标签）。
- 任务：模型通过“看”许多已知答案的数据，学习如何从输入预测出正确的答案。
- 例子：假设我们有许多水果的数据，比如重量、颜色、形状等，以及每个水果的名称（苹果、香蕉等）。模型可以学会识别这些水果的类型，这就是分类问题。另一个例子是通过历史房价数据（房屋面积、位置、房龄等），预测某个房子的价格，这就是回归问题。

总结：有监督学习就是“教”模型用例题来学习解决问题。

### 3.2 无监督学习

- 数据：无监督学习只用到输入数据，没有标签。标签并不知道“正确答案”，它要自己从数据中找到模型。
- 任务：模型的目标是找到数据中的“群体”或“隐藏结构”。
- 例子：假设有许多客户的消费数据（购买频率、购买金额等），模型可以自动将这些客户分成不同群体，比如“高价值客户”、“一般客户”、“潜力客户”，这叫聚类。

总结：无监督学习更像是“发现”数据的隐藏特征，而不是预测。

## 4. 机器学习和深度学习

机器学习和深度学习的区别主要在于他们处理数据的方式和复杂程度。

1. 数据处理和特征工程：在传统的机器学习中，我们需要对数据进行很多“特征工程”（Feature Engineering），就是手动挑选和提取数据中有用的信息，这样模型才能更好地识别规律。比如，想要通过机器学习来判断一张图片中的物体是什么，需要事先告诉模型这张图片的形状、颜色、边缘等特征。
2. 自动特征提取：而在深度学习中，模型会自动从数据中学习出有用的特征。这种自动学习的方式得益于深度神经网络的结构，可以层层提取更复杂、更抽象的信息。深度学习模型通常需要更大量的数据和计算资源，但当数据充足时，它们可以自动找到图片、语音、文本等信息中的复杂特征，而不需要人工干预。这也是为什么深度学习特别适合图像识别、语音识别等任务。
3. 层级结构：深度学习以来深层神经网络（包含很多层的神经元），而传统机器学习算法则比较简单，层数少，计算不如深度学习复杂。可以理解为，深度学习的模型像是建造了一栋“高楼大厦”，每一楼层都在处理不同层次的信息，而传统机器学习更像是一层平方，只能处理相对简单的特征。

简单说来，深度学习就是机器学习的一种“升级版”，它能更自动化、更深入地理解复杂的数据结构，但代价是需要更多的数据和计算能力。

## 5. 神经网络

神经网络是一种模拟人脑神经系统结构的算法，用于处理复杂的非线性问题。其核心理念是通过多个简单的计算单元（即神经元）组成网络，并通过多层结构和大量训练数据，使模型逐渐学习到复杂的特征和模式。

### 5.1 神经元

- 神经元（Neuron），又称为结点或单元，是神经网络的基本构成单元。每个神经元接受输入，执行加权求和，然后通过一个激活函数产生输出。

- 数学上，神经元的输出可以表示为：
    $$
    y=f(\sum^n_{i=1}w_ix_i+b)
    $$
    其中 $x_i$ 是输入，$w_i$ 是权重，$b$ 是偏置项，$f$ 是激活函数。

### 5.2 激活函数

- 激活函数用于引入非线性，使网络可以处理复杂的非线性问题。
- 常见的激活函数包括：
    - Sigmoid 函数：输出值在 (0, 1) 之间，常用于二分类问题。
    - ReLU 函数：在正值区间输入与输出相等，负值区间输出 0，使得模型更高效。
    - Tanh 函数：输出在 (-1, 1) 之间，效果类似于 Sigmoid，但更适合于归一化的数据。

### 5.3 层结构

- 神经网络一般由输入层、隐藏层和输出层组成。
    - 输入层：接收输入数据，每个节点代表一个输入特征。
    - 隐藏层：位于输入和输出层之间，可以有多层，层数越多，网络越深，称为深度神经网络（DNN）。隐藏层中的神经元通过权重和激活函数计算特征。
    - 输出层：生成最终的预测结果，输出层节点的数量取决于任务（如分类的类别数）。

### 5.4 前向传播

- 前向传播（Forward Propagation）是指数据从输入层经过隐藏层逐层传递到输出层的过程。每层的输出作为下一层的输入，直至生成最终预测。

### 5.5 损失函数

- 损失函数（Loss Function）用于评估模型预测值和真实值之间的差距。
- 常见的损失函数包括：
    - 均方误差（MSE）：用于回归任务，计算预测值与真实值之间的平方差。
    - 交叉熵损失：用于分类任务，尤其是多类分类问题。

### 5.6 反向传播

- 反向传播（Backpropagation）是训练神经网络的核心算法。它通过梯度下降优化权重和偏置，以最小化损失函数。
- 反向传播算法从输出层开始，逐层计算损失对每个权重的导数，并利用链式法则想输入层反传这些误差，更新神经网络中的权重。

### 5.7 优化算法

- 优化算法决定了如何更新权重以最小化损失。
- 常见的优化算法有：
    - 梯度下降（SGD）：逐步沿着损失函数的梯度方向更新权重。
    - Adam：一种自适应学习率优化算法，结合了动量和 RMSprop 的优点，更适合复杂问题。

### 5.8 训练和测试

- 神经网络的训练数据分为训练集和测试集。
- 在训练集中调整权重，通过测试集评估模型的泛化能力，以避免过拟合（在训练集上表现好但在新数据上效果差）。

### 5.9 深度学习与卷积神经网络（CNN）

- 当网络层数较多时，我们通常称之为深度神经网络（Deep Neural Network，DNN）。
- 卷积神经网络（Convolutional Neural Network，CNN）是深度学习的一个重要模型，尤其适合处理图像数据。CNN 通过卷积操作提取空间特征，并减少参数量，提高计算效率。



## 6. 图神经网络

图神经网络（Graph Neural Networks，GNNs）是一类能够处理图结构数据的深度学习模型。图结构数据在许多实际问题中都很常见，比如社交网络、分子结构、推荐系统等。在这些问题中，数据不仅仅是简单的特征，而是通过图中的节点和边进行连接和相互作用的。

### 6.1 图的基本概念

- 节点（Node）：图中的每个实体，可能包含一组特征。
- 边（Edge）：连接节点的关系，边也可以有特征。边的类型可以是有向或无向，权重可有可无。
- 邻接矩阵（Adjacency Matrix）：表示节点间连接关系的矩阵，通常用 0 和 1 表示是否有边连接。

### 6.2 图神经网络的基本思想

GNN 的核心思想是通过节点间的连接（即图的结构）进行信息传递。每个节点的特征向量会通过图的结构传播，经过多个图卷积层（Graph Convolution Layers），最终得到节点的嵌入表示。

在 GNN 中，每个节点的表示不仅仅依赖于其本身的特征，还会综合考虑邻居节点的信息。

### 6.3 图神经网络的常见模型

- GCN（Graph Convolutional Network）：GCN 通过对节点邻域进行卷积操作来更新节点的特征。在每一层，节点的特征表示通过其邻居节点的特征进行加权求和，并通过激活函数进行非线性变换。
- GAT（Graph Attention Network）：GAT 引入了注意力机制（Attention Mechanism），通过自适应地调整每个邻居节点的权重，来聚合邻居节点的信息。这使得网络能够根据不同邻居节点的重要性进行加权。
- GraphSAGE（Graph Sample and Aggregation）：GraphSAGE 通过对每个节点的邻居进行采样，并使用不同的聚合方法（如平均池化、LSTM、池化等）来更新节点的表示。这对于大规模图特别有用，因为它避免了全图计算的昂贵开销。
- Graph lsomorphism Network（GIN）：GIN 通过加强信息聚合的方式来增强模型的表达能力。它通过一个特定的聚合函数（如最大池化）来确保对图同构的敏感性，从而提高模型的区分能力。

### 6.4 图神经网络的应用

- 节点分类（Node Classification）：根据节点的特征和图的结构来预测节点的标签。例如：社交网络中预测用户的兴趣标签。
- 图分类（Graph Classification）：将整个图作为输入，预测图的类别。例如，在化学分子中，预测某一分子的性质。
- 链接预测（Link Prediction）：预测图中两个未连接节点是否会发生连接，常用于推荐系统和社交网络。
- 图生成（Graph Generation）：生成新的图结构，应用于药物发现、分子生成等领域。

### 6.5 GNN 的优势与挑战

- 优势：
    - 能够直接处理图结构数据，不需要将图转换为其他形式（如矩阵或向量）。
    - 能够利用图结构中的节点与边之间的关系进行学习，提高模型的表现。
    - 适用于多种类型的图数据，如无向图、有向图、加权图等。
- 挑战：
    - 扩展性：对于大规模图（如社交网络、知识图谱等），GNN 的训练和推理时间可能非常长。
    - 过平滑问题：当图神经网络层数过多时，节点可能变得过于相似，导致信息丢失。
    - 图的异质性：在实际应用中，图结构可能具有多种不同的节点和边类型，这使得传统的 GNN 模型难以适应。



## 7. 神经网络和图神经网络的区别

### 7.1 数据的形状

- 神经网络（NN）：想象你要分类一张图片，或者预测一个时间序列。神经网络通常处理的是由固定格式的数据，比如一张图片就是一个矩阵（二维），时间序列是一个顺序排列的数字。这些数据通常是一个“规则”的结构。
- 图神经网络（CNN）：图神经网络是用来处理图形数据的。你可以把图看成是一堆点（节点）通过线（边）连接起来的网络。这些点和线的关系可能很负责，不一定是按某种顺序排列的，节点之间的关系不一定是规则的。所以，GNN 适用于像社交网络、分子结构、交通网络这些更复杂、更灵活的“网状”结构。

### 7.2 信息是如何流动的

- 神经网络（NN）：在普通神经网络里，信息从一层传到下一层。每一层的输出是上一层的计算结果，所以是按照一定的顺序和规则传播的。
- 图神经网络（GNN）：图神经网络里的信息流动方式更“灵活”。每个节点不仅知道自己的一些信息，还能“向周围的朋友”了解信息（即它的邻居节点）。节点之间通过连接（边）进行信息交换。比如，社交网络里的一个人不仅受自己的兴趣影响，还受他朋友的兴趣影响，这就是图神经网络在工作。

### 7.3 什么时候使用它们

- 神经网络（NN）：当你处理的是规则的数据（比如图像、文本等），而且这些数据之间的关系比较简单时，普通的神经网络就能很好的工作。例如，分类图片、预测股票价格、语音识别等。
- 图神经网络（GNN）：当你处理的是网络或图状的数据时，比如社交网络、网页连接、推荐系统或者分子的结构等，GNN 就非常有用了。因为这些数据中，点和店之间的关系很复杂，并且没有固定顺序。比如在社交网络中，一个人的兴趣不仅由他自己决定，还受他朋友兴趣的影响，这时候 GNN 就很适合。

### 7.4 举例

- 神经网络（NN）：你有一个电子表格，每一列是一个不同的特征，比如人的身高、体重、年龄等。你想通过这些信息预测一个人的健康状况，可以通过普通的神经网络来做这几那是。
- 图神经网络（GNN）：假设你有一个社交网络图，每个人是一个节点，节点之间的边代表他们的关系（比如是朋友）。你想通过每个人的兴趣和他们朋友的兴趣来推荐音乐。这时候，你就需要用图神经网络，来处理这些复杂的关系。

总结一下，神经网络（NN）处理的是结构化的、规则的数据，而图神经网络（GNN）专门处理节点之间有复杂关系的数据，像社交网络、交通系统这样的数据。

## 8. 卷积操作

简单来说，卷积操作就像是“扫描”和“提取”图像中的重要特征。

想象一下，你有一张图片，而卷积核就像是一块小窗户，你将这个窗户放到照片的某个部分，然后通过它来看图像中的细节。这个窗户每次只查看一小部分图像（通常是一个 3 × 3 的小区域）。接下来，卷积核会“扫描”这块区域，计算窗户中的每个小部分与图像中对应部分的相互作用，比如他们的颜色、亮度等信息，然后生成一个新的数字来代表这个区域的特征。

**具体操作：**

1. 卷积核（比如一个 3 × 3 的小矩阵）就像是一个“模板”，它用来在图像上扫描并“检测”某些特征。
2. 滑动过程：这个小矩阵从图像的一角开始，像扫描仪一样划过整个图像，每次覆盖不同的区域。
3. 计算：在每个位置，它会“分析”覆盖的部分，计算这个区域内每个像素举卷积核中的相应值相乘的结果，并求和。
4. 生成新图像：计算结果会放到新的图像中，最终得到一个新的、提取了特征的图像。

通过这个过程，卷积帮助网络“看懂”图像中的有用信息，并且是自动学习的，比如识别图像中的边缘、线条、颜色等，从而让计算机能像人一样识别图像中的物体。

**例子：**

假设你有一张图片，卷积核就像是一个专门用来查找边缘的工具。如果你把它放到图像上，它会帮助你发现哪里有边缘（比如物体的轮廓）。然后，网络会把这些边缘信息传递到下一个步骤，逐步构建出对整个物体的理解。

总结来说，卷积操作是计算机“看图”和“提取有用信息”的方法，它能帮助计算机找到图像中的关键部分，从而进行更复杂的理解和分类。

























