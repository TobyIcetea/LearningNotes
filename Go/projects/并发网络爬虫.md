# 并发网络爬虫

### 项目名称：并发网络爬虫

#### 项目描述

设计一个简单的并发网络爬虫。该程序将启动多个 goroutine 来并行抓取给定网站的页面，使用 channel 进行数据通信。每个 goroutine 将从一个页面开始抓取内容，并解析页面中的链接。爬虫需要追踪已经访问过的链接，以避免重复抓取。

#### 功能要求

1. **启动入口**：用户提供一个初始 URL 列表和爬取的深度（例如，2 层深度）。
2. **链接去重**：维护一个数据结构来记录已经访问过的链接，避免重复爬取。
3. **并发爬取**：使用 goroutine 并发抓取网页内容。每个 goroutine 负责一个链接的爬取，爬取完成后将该链接的页面内容和其找到的链接传回主 goroutine。
4. **channel 通信**：使用 channel 来传递 goroutine 的抓取结果给主 goroutine。可以有两个 channel，一个用于传输抓取的页面内容，另一个用于传输解析到的链接。
5. **结果输出**：在抓取完成后，将所有页面的内容保存到本地文件中，并输出所有抓取到的链接列表。
6. **错误处理**：处理爬取过程中可能遇到的错误，例如网络错误、解析错误等。

#### 拓展功能（可选）

- **抓取频率控制**：设置一个最大并发数，以控制 goroutine 的数量。
- **超时控制**：对爬取请求设置超时，以防止某些网页长时间无响应。
- **深度限制**：支持用户指定抓取深度，超出该深度的链接不再抓取。
- **网页内容过滤**：只抓取特定内容（如包含关键词的页面）。

### 提示

- 你可以使用 `sync.WaitGroup` 来跟踪所有 goroutine 的完成情况。
- 使用 `map` 和 `sync.Mutex` 来记录已访问的 URL。
- 可以利用 `time` 包来设定请求超时和爬取频率。

### 代码

```go
package main

import (
    "fmt"
    "io/ioutil"
    "net/http"
    "regexp"
    "sync"
    "time"
)

// fetch 函数从给定的 URL 获取页面内容
func fetch(url string) (string, error) {
    // 设置超时时间
    client := &http.Client{Timeout: 10 * time.Second}

    // 发起 GET 请求
    resp, err := client.Get(url)
    if err != nil {
       return "", fmt.Errorf("无法获取页面内容：%v", err)
    }
    defer resp.Body.Close()

    // 读取页面内容
    body, err := ioutil.ReadAll(resp.Body)
    if err != nil {
       return "", fmt.Errorf("无法读取页面内容：%v", err)
    }

    return string(body), nil
}

// extractLinks 函数，从页面内容中提取链接
func extractLink(content string) []string {
    // 使用正则表达式匹配 href 属性中的 URL
    re := regexp.MustCompile(`href="(http[s]?://[^\s"<>]+)"`)
    matches := re.FindAllStringSubmatch(content, -1)

    var links []string
    for _, match := range matches {
       links = append(links, match[1])
    }
    return links
}

// worker 函数，负责从任务队列中获取 URL 并抓取内容
func worker(id int, urls <-chan string, results chan<- string, visited *sync.Map, wg *sync.WaitGroup, depth int, maxDepth int) {
    defer wg.Done()
    for url := range urls {
       if depth > maxDepth {
          continue
       }
       // 检查 URL 是否已经被访问过
       if _, ok := visited.LoadOrStore(url, true); ok {
          continue
       }

       fmt.Printf("Worker %d 开始抓取：%s\n", id, url)
       content, err := fetch(url)
       if err != nil {
          fmt.Printf("Worker %d 抓取失败：%s\n", id, err)
          continue
       }
       results <- fmt.Sprintf("URL: %s\n\n%s\n", url, content)

       // 解析页面中的链接
       links := extractLink(content)
       for _, link := range links {
          // 递归抓取新的链接，深度加一
          go func(link string) {
             worker(id, make(chan string), results, visited, wg, depth+1, maxDepth)
          }(link)
       }
    }
}

func main() {
    // 待抓取的 URL 列表
    urls := []string{
       "https://www.baidu.com",
    }

    // 创建任务和结果通道
    urlChannel := make(chan string)
    resultChannel := make(chan string)

    // 使用 WaitGroup 追踪 goroutine 完成情况
    var wg sync.WaitGroup
    visited := &sync.Map{} // 最终已访问的 URL

    // 最大抓取深度
    maxDepth := 2

    // 启动 3 个 worker
    for i := 1; i <= 3; i++ {
       wg.Add(1)
       go worker(i, urlChannel, resultChannel, visited, &wg, 1, maxDepth)
    }

    // 启动一个 goroutine 来关闭 resultChannel，当所有抓取完成时
    go func() {
       wg.Wait()
       close(resultChannel)
    }()

    // 将 URL 发送到任务通道
    go func() {
       for _, url := range urls {
          urlChannel <- url
       }
       close(urlChannel)
    }()

    // 打印抓取结果
    for result := range resultChannel {
       fmt.Println("抓取结果：\n", result)
    }

}
```
