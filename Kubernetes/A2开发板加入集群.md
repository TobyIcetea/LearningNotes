# A2 开发板加入集群

## 1. 主机规划

规划好主机的 IP 地址。本次我们使用一个虚拟机作为 master 节点，一个虚拟机作为 worker 节点，加上一个 Atlas 200I DK A2 作为另一个 worker 节点。

整体的网段规划都在 `192.168.137.*` 网段，具体说，我们使用的三台主机的 IP 地址规划如下：

| 节点名 | IP地址          |
| ------ | --------------- |
| master | 192.168.137.101 |
| node   | 192.168.137.102 |
| atlas  | 192.168.137.100 |

## 2. 环境初始化

master 节点和 node 节点的配置在另一个文档中已经描述得比较清楚了，这里主要是考虑如何将开发板加入集群。这其中主要涉及到两方面的问题：

- 开发板是 arm64 架构的
- 一些软件需要按照 Ascend 的规范

所以下面讲的都是对开发板的配置：

1. 主机名解析。

    为了方面后续集群节点之间的直接调用：

    ```bash
    hostnamectl set-hostname atlas
    
    vim /etc/hosts
    # 加入下面的内容
    ---------------------------------------
    192.168.137.100		atlas
    192.168.137.101		master
    192.168.137.102		node
    ---------------------------------------
    ```

2. 时间同步。

    开发板我们使用 ntpdate 来进行时间同步。

    ```bash
    # 先同步一次时间
    ntpdate time1.aliyun.com
    
    # 添加 crontab 更新时间
    cat << EOF >> /etc/crontab
    0 * * * * ntpdate time1.aliyun.com
    EOF
    
    # 设定时区
    timedatectl set-timezone Asia/Shanghai
    
    # 验证时间对不对
    date
    ```

3. 禁用 iptables 服务和 firewalld 服务。

    ```bash
    这里什么也不用做，因为开发板本来就没有开启这些模块。
    ```

4. 禁用 SELINUX。

    ```bash
    这里也是啥也不用做。
    ```

5. 禁用 swap 分区。

    ```bash
    # 开发板或许是做了什么特殊的设置，在 /etc/fstab 中禁用是不起作用的
    # 但是我们还是改一下，给 /etc/fstab 中所有行前面都加一个 #
    sed -i 's/^/#/' /etc/fstab
    
    swapon --show
    # 发现 swap 分区还是打开的
    
    # 创建 service，让我们每次开机之后都执行一次手动关闭 swap 分区的操作
    cat << EOF > /etc/systemd/system/swapoff.service
    [Unit]
    Description=Disable swap on boot
    
    [Service]
    Type=oneshot
    ExecStart=/sbin/swapoff -a
    
    [Install]
    WantedBy=multi-user.target
    EOF
    
    # 开启服务
    systemctl start swapoff
    systemctl enable swapoff
    ```

6. 修改 Linux 内核参数。

    ```bash
    cat << EOF >> /etc/sysctl.d/kubernetes.conf
    net.bridge.bridge-nf-call-ip6tables = 1
    net.bridge.bridge-nf-call-iptables = 1
    net.ipv4.ip_forward = 1
    EOF
    
    # 重新加载配置
    sysctl -p
    
    # 加载网桥过滤模块
    # 将加载模块的配置写入文件中，这样可以永久生效
    echo "br_netfilter" | sudo tee /etc/modules-load.d/br_netfilter.conf && sudo modprobe br_netfilter
    
    # 查看网桥过滤模块是否加载成功
    lsmod | grep br_netfilter
    ```

7. 配置 ipvs 功能。

    ```bash
    # 安装 ipset 和 ipvsadm
    yum install ipset ipvsadm -y
    
    # 添加需要加载的模块写入脚本文件
    mkdir /etc/sysconfig/modules
    cat <<EOF >  /etc/sysconfig/modules/ipvs.modules
    #!/bin/bash
    modprobe -- ip_vs
    modprobe -- ip_vs_rr
    modprobe -- ip_vs_wrr
    modprobe -- ip_vs_sh
    modprobe -- nf_conntrack
    EOF
    
    # 为脚本文件添加执行权限
    chmod +x /etc/sysconfig/modules/ipvs.modules
    
    # 执行脚本文件
    /bin/bash /etc/sysconfig/modules/ipvs.modules
    
    # 查看对应的模块是否加载成功
    lsmod | grep -e ip_vs -e nf_conntrack
    ```

8. 重启开发板。

    ```bash
    reboot
    ```

## 3. 安装 containerd

这里我们使用了最新版的 cri-containerd-cni 包进行安装：

```bash
wget https://github.com/containerd/containerd/releases/download/v1.7.26/cri-containerd-cni-1.7.26-linux-arm64.tar.gz

mkdir cri-containerd
tar zxvf cri-containerd-cni-1.7.26-linux-arm64.tar.gz -C cri-containerd
cd cri-containerd
rsync -av ./etc/ /etc/
rsync -av ./opt/ /opt/
rsync -av ./usr/ /usr/

# 删除 /etc/cni/net.d 目录下的自带的文件
# 在部署 k8s 集群的时候，这个文件夹下面是不需要有文件的
# 自带的这个文件会让 pod 网络在另一个网段上
rm /etc/cni/net.d/10-containerd-net.conflist
```

生成 containerd 的配置文件：

```bash
mkdir /etc/containerd

containerd config default > /etc/containerd/config.toml
vim /etc/containerd/config.toml
---------------------------------------------------------------------------------
# 如果要配置 K8s 的话，就要修改这里
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
  SystemdCgroup = true
# 如果容器启动不起来，爆出 sandbox_image 之类的东西，可以修改这里
  sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.9"
# 修改镜像加速文件地址
  config_path = "/etc/containerd/certs.d"
---------------------------------------------------------------------------------

# 加载配置、启动
systemctl daemon-reload
systemctl enable containerd
systemctl start containerd
```

配置镜像加速地址：

```bash
# docker hub镜像加速
mkdir -p /etc/containerd/certs.d/docker.io
cat > /etc/containerd/certs.d/docker.io/hosts.toml << EOF
server = "https://docker.io"
[host."https://1ecf599359e64520bd04701e6d7184e8.mirror.swr.myhuaweicloud.com"]
  capabilities = ["pull", "resolve"]

[host."https://le2c3l3b.mirror.aliyuncs.com"]
  capabilities = ["pull", "resolve"]

[host."https://docker.m.daocloud.io"]
  capabilities = ["pull", "resolve"]

[host."https://reg-mirror.qiniu.com"]
  capabilities = ["pull", "resolve"]

[host."https://registry.docker-cn.com"]
  capabilities = ["pull", "resolve"]

[host."http://hub-mirror.c.163.com"]
  capabilities = ["pull", "resolve"]

EOF

# registry.k8s.io镜像加速
mkdir -p /etc/containerd/certs.d/registry.k8s.io
tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml << 'EOF'
server = "https://registry.k8s.io"

[host."https://k8s.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# docker.elastic.co镜像加速
mkdir -p /etc/containerd/certs.d/docker.elastic.co
tee /etc/containerd/certs.d/docker.elastic.co/hosts.toml << 'EOF'
server = "https://docker.elastic.co"

[host."https://elastic.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# gcr.io镜像加速
mkdir -p /etc/containerd/certs.d/gcr.io
tee /etc/containerd/certs.d/gcr.io/hosts.toml << 'EOF'
server = "https://gcr.io"

[host."https://gcr.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# ghcr.io镜像加速
mkdir -p /etc/containerd/certs.d/ghcr.io
tee /etc/containerd/certs.d/ghcr.io/hosts.toml << 'EOF'
server = "https://ghcr.io"

[host."https://ghcr.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# k8s.gcr.io镜像加速
mkdir -p /etc/containerd/certs.d/k8s.gcr.io
tee /etc/containerd/certs.d/k8s.gcr.io/hosts.toml << 'EOF'
server = "https://k8s.gcr.io"

[host."https://k8s-gcr.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# mcr.m.daocloud.io镜像加速
mkdir -p /etc/containerd/certs.d/mcr.microsoft.com
tee /etc/containerd/certs.d/mcr.microsoft.com/hosts.toml << 'EOF'
server = "https://mcr.microsoft.com"

[host."https://mcr.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# nvcr.io镜像加速
mkdir -p /etc/containerd/certs.d/nvcr.io
tee /etc/containerd/certs.d/nvcr.io/hosts.toml << 'EOF'
server = "https://nvcr.io"

[host."https://nvcr.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# quay.io镜像加速
mkdir -p /etc/containerd/certs.d/quay.io
tee /etc/containerd/certs.d/quay.io/hosts.toml << 'EOF'
server = "https://quay.io"

[host."https://quay.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# registry.jujucharms.com镜像加速
mkdir -p /etc/containerd/certs.d/registry.jujucharms.com
tee /etc/containerd/certs.d/registry.jujucharms.com/hosts.toml << 'EOF'
server = "https://registry.jujucharms.com"

[host."https://jujucharms.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# rocks.canonical.com镜像加速
mkdir -p /etc/containerd/certs.d/rocks.canonical.com
tee /etc/containerd/certs.d/rocks.canonical.com/hosts.toml << 'EOF'
server = "https://rocks.canonical.com"

[host."https://rocks-canonical.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF
```

## 4. 安装 k8s 组件

```bash
# 由于 K8s 的镜像源在国外，速度比较慢，这里换成国内的镜像源
cat << EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-aarch64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 安装 kubeadm、kubelet 和 kubectl
# 这里安装的是 kubelet 1.28.2 版本
yum install kubeadm-1.28.2-0 kubelet-1.28.2-0 kubectl-1.28.2-0 -y

# 编辑 kubelet 的 cgroup
cat << EOF > /etc/sysconfig/kubelet
KUBELET_CGROUP_ARGS="--cgroup-driver=systemd"
KUBE_PROXY_MODE="ipvs"
EOF

# 设置 kubelet 开机自启动
systemctl enable kubelet
```

## 5. 加入集群

在 master 节点上获取节点加入的命令：

```bash
[root@master ~]# kubeadm token create --print-join-command
kubeadm join 192.168.137.101:6443 --token dblf2u.9fd6ek5dwvz4cf38 --discovery-token-ca-cert-hash sha256:9e72ff25130bd6ca7e41dd7e40f87577456e5be771aa38ae0cc00b266ce3ddfd 
```

之后在我们的开发板上执行这个 join 的命令，就可以加入集群了。

## 6. 验证

```bash
# 创建 pod 文件
cat << EOF > nginx-pod.yaml
apiVersion: v1
kind: Pod 
metadata:
  name: nginx-demo
spec:
  containers:
    - name: nginx
      image: nginx:latest
  nodeName: atlas
EOF

# 启动 pod
kubectl apply -f nginx-pod.yaml

# 等一会儿之后就可以看到 pod 跑起来了
[root@master ~]# kubectl get pods -o wide
NAME          READY   STATUS    RESTARTS      AGE   IP           NODE     NOMINATED NODE   READINESS GATES
nginx-demo    1/1     Running   0             45s   10.244.4.2   atlas    <none>           <none>


```





























