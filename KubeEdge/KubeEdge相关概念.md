# KubeEdge 相关概念

## 1. cgroupfs 和 systemd

在 Linux 系统中，`cgroupfs` 和 `systemd` 是管理和控制系统资源的重要工具，尤其在容器化和资源隔离方面应用广泛。

### 1.1 cgroup(Control Groups)

cgroup 是 linux 内核的一项特性，用于限制、记录和隔离进程组的资源使用。通过 cgroups，可以控制 CPU、内存、磁盘 I/O 等资源的分配。它在资源管理、容器化（如 Docker）以及系统服务的隔离和监控中扮演了重要角色。

**【cgroupfs】**

`cgroupfs` 是通过文件系统接口访问和管理 cgroup 的一种方式。通常情况下，cgroupfs 挂载在 `/sys/fs/cgroup` 下，通过目录结构来表示会不同的 cgroup 资源控制器（如 `cpu`、`memory`、`blkio` 等），并允许用户直接操作这些文件和目录来管理资源。

**【主要功能】**

- 资源限制：可以限制 CPU、内存、网络带宽等的使用。例如，可以限制一个进程最多使用 500MB 内存。
- 优先级控制：通过设置权重，让系统资源按优先级分配给不同进程。
- 监控和计量：可以监控进程的资源使用情况。
- 进程隔离：允许把不同的进程组放入不同的 cgroup 中，实现资源隔离。

**【cgroup 版本】**

cgroup 分为 v1 和 v2 版本：

- cgroup v1：每个资源控制器有独立的子系统目录，比如：`/sys/fs/cgroup/cpu`、`/sys/fs/cgroup/memory`。
- cgroup v2：统一管理所有控制器，通过单一层次的挂载点（通常为 `/sys/fs/cgroup`）进行访问。简化了配置和管理，适合更复杂的资源调度。

### 1.2 systemd 和 cgroup

`systemd` 是 linux 系统的初始化系统和服务管理器。它在管理系统启动、服务、进程以及资源控制方面非常强大。`systemd` 内部集成了对 cgroup 的支持，可以通过配置文件直接使用 cgroup 的功能。

**【systemd 与 cgroup 的集成】**

- 自动创建 cgroup：在 `systemd` 启动服务时，它会为每个服务创建一个独立的 cgroup。例如，当你启动一个名为 `httpd` 的服务时，`systemd` 会在 `/sys/fs/cgroup` 中自动创建一个相应的目录（如 `/sys/fs/cgroup/system.slice/httpd.service`）。
- 资源管理：可以直接在 `systemd` 单元文件中定义资源限制，例如内存限制、CPU 限制等。`systemd` 会自动将这些配置映射到 cgroup 中。
- 状态监控：`systemd` 提供了 `systemctl` 和 `journalctl` 命令，用于监控服务的状态和资源使用情况。

【systemd 相关的配置实例】

在 `systemd` 服务单元文件中，可以直接使用资源控制指令，如下示例：

```ini
[Service]
# 限制服务的内存使用量上限为 512M
MemoryMax=512M

# 限制服务的 CPU 使用率为 50%
CPUQuota=50%

# 将服务配置到一个独立的 CPU 集群组
CPUAffinity=1 2 3

# 限制服务的 I/O 带宽为 10M/s
IOReadBandwidthMax=/dev/sda 10M
IOWriteBandwidthMax=/dev/sda 10M
```

通过这种方式，可以非常方便地管理服务和进程的资源使用。

### 1.3 systemd-cgls 和 systemd-cgtop

`systemd` 还提供了一些命令来查看和管理 cgroup 资源：

- systemd-cgls：这个命令可以显示当前 cgroup 的层次结构。执行 `systemd-cgls` 会以树状图的形式列出所有正在运行的服务和进程，帮助了解服务和进程的层级关系。
- systemd-cgtop：它可以实时监控 cgroup 资源使用情况，类似于 `top` 命令。通过它可以查看各个服务的 CPU、内存、I/O 使用情况。

### 1.4 使用实例

下面是一些使用实例，演示如何通过 `systemd` 和 cgroup 管理系统资源：

**【创建一个新的 cgroup】**

通过 `cgroupfs` 手动创建：

```bash
# 创建一个名为 "test" 的 cgroup
mkdir /sys/fs/cgroup/cpu/test
# 把进程 ID 为 1234 的进程添加到 "test" cgroup 中
echo 1234 > /sys/fs/cgroup/cpu/test/tasks
# 设置 CPU 使用限制，例如限制 CPU 使用为 20%
echo 20000 > /sys/fs/cgroup/cpu/test/cpu.cfs_quota_us
```

通过 `systemd` 创建：

1. 创建一个服务文件，比如 `my_test.service`：

    ```ini
    [Unit]
    Description=My Test Service
    
    [Service]
    ExecStart=/usr/bin/sleep infinity
    MemoryMax=256M
    CPUQuota=20%
    
    [Install]
    WantedBy=multi-user.target
    ```

2. 启动该服务：

    ```bash
    systemctl daemon-reload
    systemctl start my_test.service
    ```

### 1.5 总结

- `cgroupfs` 提供了文件系统接口，可以直接操作文件来控制和监控资源。
- `systemd` 集成了对 cgroup 的管理，使得在服务管理中应用 cgroup 更加简便，并通过单元文件实现资源限制。

这两者配合使用，使得 Linux 系统的资源管理和进程控制更加高效和便捷，适用于容器化、云计算等现代应用场景。

## 2. 高可用部署

KubeEdge 是一个基于 Kubernetes 的开源计算平台，主要用于实现云端和边缘之间的高效协作。在高可用（High Availability，HA）部署方面，KubeEdge 通常需要实现对边缘节点和云端控制平面（Kubernetes Master）组件的冗余，以避免单点故障，确保系统稳定性。以下是 KubeEdge 高可用部署的关键点。

### 2.1 云端控制平面的高可用

- 在 KubeEdge 的架构中，云端控制平面（即 Kubernetes Master）主要管理和调度边缘节点的工作负载。为了实现高可用，通常需要在云端部署多个 Master 节点。
- 使用负载均衡器（如 Nginx 或其他云提供商的负载均衡服务）将请求分发到多个 Master 节点，这样即使某个 Master 节点发生故障，其他节点也能继续提供服务。
- Kubernetes 本身的组件（如 etcd、API Servier、Controller Manager、Scheduler）可以配置为多实例模式，以确保高可用性和数据一致性。

### 2.2 边缘节点的高可用

- 在边缘节点上，KubeEdge 的 EdgeCore 负责与云端通信、管理设备、处理本地容器生命周期等。
- EdgeCore 的高可用可以通过在多个物理或虚拟节点上运行多个实例来实现。如果某个边缘节点离线或发生故障，工作负载可以重新调度到其他可用的边缘节点。
- KubeEdge 支持边缘自治模式，即边缘节点可以在断网的情况下依旧执行本地工作负载的原理，这增强了边缘节点的容错能力。

### 2.3 KubeEdge 组件的高可用

- KubeEdge 的主要组件包括云端的 CloudCore 和边缘的 EdgeCore。实现高可用时，需要确保 CloudCore 的高可用，以保证边缘节点能够正常与云端通信。
- 在云端，部署多个 CloudCore 实例并配置负载均衡器来分发请求可以增加 CloudCore 的可用性。
- 对于 EdgeCore，由于它运行在边缘设备上，通常可以直接在每个边缘设备上独立部署。如果某个 EdgeCore 节点发生故障，其他节点可以继续正常工作。

### 2.4 网络连接的高可用

- 边缘节点与云端通过网络连接进行通信。为了确保高可用，通常需要部署冗余网络路径，避免由于单一路径故障导致通信中断。
- 通过使用 VPN、专用网络链路等技术，可以提高云端与边缘节点之间网络连接的可靠性。

### 2.5 数据持久化的高可用

- 在 Kubernetes 的 etcd 数据库中存储了集群的配置信息和状态数据。确保 etcd 数据的高可用可以采用多节点的 etcd 集群，确保即使某个节点发生故障，数据依然可用。
- 对于 KubeEdge 边缘节点的本地数据，可以使用持久化存储（如 NFS、GlusterFS 或 Ceph）来实现高可用。

### 2.6 总结

在实际部署过程中，KubeEdge 高可用通常会包含以下几项配置：

- 多实例的 Kubernetes Master 和 CloudCore 以实现云端的高可用。
- 冗余的边缘节点部署和 EdgeCore 部署以确保边缘端的稳定运行。
- 通过负载均衡、VPN 和冗余网络连接等手段提高整体的可靠性和容错性。

这些措施能帮助构建一个更具容错性和可扩展性的边缘计算平台，适用于多种应用场景，包括智能制造、物联网和智慧城市等。

## 3. 容器化部署和二进制部署

### 3.1 容器化部署

容器化部署是指使用「容器」来打包应用程序及其所有依赖项的过程。最常用的容器化技术就是 Docker。容器本质上是一个独立、可移植的打包单位，它包含了应用程序的代码、依赖库、配置文件等等。这种方法在云端特别受欢迎，主要原因如下：

1. 隔离性：每个容器运行在自己独立的环境中，不会影响其他容器。
2. 可移植性：容器可以在任何支持容器运行的环境下都可以直接运行，消除了「在我机器上没问题」的情况。
3. 资源利用效率：容器不像虚拟机那样需要完整的操作系统，它们直接共享主机的内核资源，因此启动速度更快，消耗资源更少。
4. 版本一致性：容器镜像中打包的内容不会岁外部环境改变，从而保证了开发、测试和生产环境的一致性。

### 3.2 二进制部署

二进制部署指的是直接将编译好的应用程序二进制文件部署到目标服务器上。这种方法通常包括以下步骤：开发人员编写代码并编译为二进制文件，然后将这些文件（以及必要的依赖）拷贝到服务器上运行。这种方式的特点是：

1. 直接性：没有额外的抽象层，直接把应用程序放到服务器上运行。
2. 依赖问题：需要确保目标服务器具备应用运行所需的所有依赖项（例如，正确的库、配置环境等）。
3. 环境一致性问题：不同环境之间的差异可能会导致应用在不同服务器上表现不一致，这也是「它在我机器上没问题」的常见来源。
4. 维护成本高：需要手动管理和维护应用的依赖项，更新版本时可能面临依赖不兼容的问题。

### 3.3 区别总结

- 环境一致性：容器化部署更容易实现一致性，二进制部署可能面临环境差异带来的问题。
- 资源使用：容器更节约资源，特别适合云端和微服务架构。二进制部署则更适合小规模、资源充足的环境。
- 易用性和灵活性：容器化部署更灵活、易于扩展和管理，特别是在大规模分布式系统中优势明显。二进制部署则在简单、传统的项目中更为直接。

容易化部署在云计算、云原生领域已经成为主流，像 Kubernetes 这样的容器编排工具也提供了强大的集群管理能力。

## 4. Device Twin

在 KubeEdge 中，Device Twin 是一个用于管理物联网设备（IoT 设备）状态的概念。它是一种虚拟表示，存储设备的属性、状态和控制信息，能够提供与物理设备的双向通信接口。Device Twin 主要用于设备管理、监控和控制，尤其是在云边协同的场景下。

### 4.1 Device Twin 的工作原理

1. 设备虚拟化：Device Twin 是设备的虚拟副本，可以存储和描述设备的属性、状态、命令等。它使得云端应用和边缘计算应用能够方便地获取和管理设备状态。
2. 属性同步：KubeEdge 通过 Device Twin 同步设备的属性和状态。在边缘设备与云端进行数据同步时，Device Twin 负责确保设备的状态和云端的设备状态保持一致。
3. 双向通信：设备的属性可以在边缘端和云端之间双向同步。比如，设备的当前状态（例如传感器数据）可以发送到云端，而设备的控制命令（例如开启设备、设置参数）则可以从云端发送到设备。
4. 脱机工作：在边缘计算环境中，设备可能与云端断开连接。Device Twin 使得设备可以在脱机的情况下继续工作，并在恢复连接时进行状态同步，确保设备的状态在云端和边缘设备之间一致。

### 4.2 Device Twin 的主要功能

- 设备状态管理：Device Twin 负责存储和更新设备的属性（如设备的型号、配置等）以及设备的实时状态（如传感器值、设备是否在线等）。
- 属性与命令同步：支持设备属性的双向同步，并能够接收来自云端的命令，更新设备的状态或控制设备的行为。
- 远程管理和监控：通过 Device Twin，用户可以从云端远程监控和管理物联网设备，获取设备的实时数据，并执行必要的控制操作。
- 异步操作：设备状态的更新和命令的执行通常是异步的，这对于支持大规模物联网设备的管理和控制非常重要。

### 4.3 KubeEdge 中 Device Twin 的应用场景

- 设备控制和管理：可以实时监控设备的状态、配置和故障信息，及时响应设备问题。比如，如果某个设备传感器的数据异常，可以立即采取措施。
- 智能家居和工业自动化：在智能家居、工业自动化等场景中，Device Twin 用于管理设别的属性和状态，实现设备的远程控制和自动化操作。
- 边缘计算场景：在边缘计算中，Device Twin 使得边缘节点可以在云端断开连接时仍然保持设备的本地管理和控制，保证设备可以继续运行，直到重新连接到云端进行同步。

### 4.4 相关组件和计数

KubeEdge 中的 Device Twin 通常与以下技术和组件结合使用：

- EdgeCore：KubeEdge 的核心组件，负责在边缘节点管理设备与云端通信。
- EdgeHub：EdgeHub 是边缘端的组件，负责设备的控制、通信和状态同步。它与云端的云 Hub 进行双向数据交换。
- Edge Controller：负责协调边缘计算和云计算之间的通信，处理设备的状态和命令。

## 5. keepalived

`keepalived` 是一个用于高可用性和负载均衡的 Linux 工具，主要用于实现虚拟路由冗余协议（VRRP）以及监控、故障转移等功能，广泛用于提升系统的可用性和可靠性，尤其是在服务和网络层面。

### 5.1 主要功能

1. 高可用性（HA）：Keepalived 常用于实现高可用集群。它使用 VRRP 协议，通过多个节点间的 IP 地址漂移，确保如果一个节点失效，另一个节点能够接管其工作，从而保证服务不中断。
2. 负载均衡：Keepalived 也支持简单的负载均衡功能，可以通过调度流量到多个后端服务器上来实现负载分担。它支持基于 IP 的负载均衡以及基于服务器健康检查的调度。
3. 健康检查：Keepalived 能够通过内置的健康检查机制监控后端服务器的健康状态。如果某个服务器不可用，Keepalived 会自动将流量重定向到健康的服务器。
4. VRRP（虚拟路由冗余协议）：VRRP 是一种用于动态调整默认网关的协议，它允许多个路由器共享一个虚拟 IP 地址。如果主机路由器宕机，VRRP 可以让其他路由器接管该虚拟 IP 地址，从而保持网络通信不中断。

### 5.2 工作原理

1. VRRP 模式：
    - 在一个 VRRP 集群中，有一个 Master 节点（主节点）和若干个 Backup 节点（备份节点）。主节点负责处理流量，而备份节点则处于待命状态。
    - 所有节点共享一个虚拟 IP 地址，客户端通过该虚拟 IP 地址访问服务。如果主节点失效，备份节点会自动接管虚拟 IP 地址，确保服务不中断。
2. Keepalived 配置：Keepalived 的配置文件通常位于 `/etc/keepalived/keepalived.conf`。配置文件主要包括以下几部分内容：
    - Global Definitions：全局配置，比如日志设置、进程名等。
    - Virtual Router：虚拟路由器配置，指定虚拟 IP 地址、优先级、接口等。
    - Health Checks：健康检查配置，指定检查目标及检查条件。
    - Virtual Server：虚拟服务器配置，通常用于负载均衡，指定池中的多个后端服务器。

### 5.3 应用场景

1. Web 服务器高可用：在 Web 服务器集群中，Keepalived 可用来管理虚拟 IP 地址，确保在一个服务器故障时，流量能够转发到其他健康的服务器。
2. 数据库高可用：在 MySQL 或 PostgreSQL 等数据库集群中，Keepalived 可以用过 VRRP 实现主从数据库切换，确保数据库服务的持续可用。
3. 负载均衡：Keepalived 与 LVS（Linux Vritual Server）结合使用时，能够实现 Layer 4 负载均衡，将流量均衡分配到后端服务器。
4. 高可用 VPN 网关：Keepalived 也可以用于 VPN 网关的高可用配置，确保用户能够始终通过虚拟网关访问企业网络。

### 5.4 Keepalived 与 KubeEdge

**【Keepalived：传统高可用性和负载均衡】**

- 高可用性：`Keepalived` 主要依赖于虚拟 IP 地址和主备节点模式来提供高可用性。通过 VRRP 协议，多个服务器（主服务器和备份服务器）共享一个虚拟 IP 地址。如果主服务器宕机，备份服务器会自动接管这个虚拟 IP 地址，继续提供服务。
- 负载均衡：`Keepalived` 还可以与 LVS（Linux Virtual Server）或其他负载均衡器结合使用，将流量分发到多态后端服务器上。它的负载均衡一般是在 4 层（传输层）工作，主要是根据 IP 和端口进行转发。

**【KubeEdge：边缘计算平台】**

- 高可用性：`KubeEdge` 是基于 Kubernetes 和 Edge Computing 的平台，它提供的高可用性方式主要通过 Pod 和 RelicaSet 实现。在 KubeEdge 上，Pod 会根据 Kubernetes 的调度策略自动在集群内重建和分配。当某个节点失效时，Kubernetes 的 ReplicaSet 会确保新 Pod 被调度到健康的节点上，实现服务的高可用性。
- 负载均衡：在 `KubeEdge` 中，负载均衡更多的是通过 Kubernetes Service 来实现的。Kubernetes 提供的 ClusterIP、NodePort 或 LoadBalancer 等服务类型可以帮助在集群中分发流量。在边缘计算场景下，流量可以通过 Ingress 或 EdgeHub 在云端和边缘设备之间进行调度和负载均衡。

### 5.5 切换到 KubeEdge 后的好处

**【高可用性的变化】**

- 不再使用传统的虚拟 IP 地址和 Keepalived 的主备机制，而是依赖于 Kubernetes 的 Pod 重调度和 ReplicaSet 来实现高可用性。

**【负载均衡的变化】**

- 不再需要使用外部的负载均衡器（如 LVS 和 HAProxy）来进行流量分发，KubeEdge 内建的 Kubernetes 服务模型和 EdgeHub 将自动处理流量的分发和负载均衡。

**【集群规模和资源管理】**

- 集群管理和扩展会更加自动化，不再需要手动管理虚拟 IP 和负载均衡器。

**【边缘计算支持】**

- KubeEdge 提供的边缘计算能力可以在远离数据中心的设备上执行计算和存储任务，而不需要依赖中心化的服务器。

### 5.6 Keepalived 和 KubeEdge 所管理的层级

`Keepalived` 本身并不涉及容器管理，它主要用于管理物理服务器或虚拟机级别的资源。在传统的基于 Keepalived 的高可用方案中，它是以 Node 为单位进行管理的，通常管理的是物理机器或虚拟机的 IP 地址、故障转移和负载均衡。

而 KubeEdge 更关注的是容器和边缘设备的管理。在 KubeEdge 中，资源管理的单元已经从物理节点扩展到容器界面，因此它不仅管理 node，还可以管理 Pod 和 Device。

总结：

- Keepalived 管理的是物理节点或虚拟机（即机器层级的资源）。
- KubeEdge 管理的是容器、节点以及与边缘设备的协同（即在云端和边缘设备之间的资源管理）。





















